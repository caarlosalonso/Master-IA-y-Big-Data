{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af6dc26",
   "metadata": {},
   "source": [
    "- **EVALUABLE 1:** Colecciones de datos\n",
    "    \n",
    "    Trabajas en el equipo de MLOps de una empresa que entrena múltiples modelos de deep learning simultáneamente.\n",
    "    Necesitas implementar un sistema de gestión de experimentos que permita rastrear, validar y analizar resultados de diferentes arquitecturas, \n",
    "    datasets y configuraciones de hiperparámetros.\n",
    "    \n",
    "    El sistema debe manejar:\n",
    "    \n",
    "    - **Configuraciones de experimentos** (diccionarios)\n",
    "    - **Historial de métricas por época** (listas)\n",
    "    - **Metadatos inmutables de modelos** (tuplas)\n",
    "    - **Validación de recursos y dependencias** (sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46604be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de datos de experimentos ejecutados\n",
    "experimentos_db = {\n",
    "    \"exp_20241101_001\": {\n",
    "        \"modelo\": \"ResNet50\",\n",
    "        \"dataset\": \"ImageNet\",\n",
    "        \"metadata\": (224, 224, 3, 1000),  # (height, width, channels, num_classes)\n",
    "        \"hiperparametros\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 32,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"epochs\": 100\n",
    "        },\n",
    "        \"metricas_entrenamiento\": {\n",
    "            \"loss\": [2.3, 1.8, 1.5, 1.2, 0.95, 0.87, 0.81, 0.78, 0.76, 0.75],\n",
    "            \"accuracy\": [0.45, 0.58, 0.65, 0.71, 0.76, 0.79, 0.82, 0.84, 0.85, 0.86]\n",
    "        },\n",
    "        \"metricas_validacion\": {\n",
    "            \"loss\": [2.4, 1.9, 1.6, 1.4, 1.1, 0.98, 0.92, 0.89, 0.88, 0.87],\n",
    "            \"accuracy\": [0.43, 0.56, 0.63, 0.69, 0.74, 0.77, 0.80, 0.82, 0.83, 0.84]\n",
    "        },\n",
    "        \"dependencias\": {\"torch\", \"torchvision\", \"numpy\", \"pillow\"},\n",
    "        \"gpu_usado\": \"NVIDIA_A100\",\n",
    "        \"tiempo_total_minutos\": 245\n",
    "    },\n",
    "    \"exp_20241101_002\": {\n",
    "        \"modelo\": \"VGG16\",\n",
    "        \"dataset\": \"CIFAR10\",\n",
    "        \"metadata\": (32, 32, 3, 10),\n",
    "        \"hiperparametros\": {\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"batch_size\": 64,\n",
    "            \"optimizer\": \"SGD\",\n",
    "            \"epochs\": 50\n",
    "        },\n",
    "        \"metricas_entrenamiento\": {\n",
    "            \"loss\": [2.1, 1.6, 1.3, 1.0, 0.85, 0.72, 0.65, 0.61, 0.58, 0.56],\n",
    "            \"accuracy\": [0.35, 0.52, 0.61, 0.68, 0.73, 0.77, 0.80, 0.82, 0.83, 0.84]\n",
    "        },\n",
    "        \"metricas_validacion\": {\n",
    "            \"loss\": [2.2, 1.7, 1.4, 1.2, 1.0, 0.88, 0.82, 0.80, 0.79, 0.78],\n",
    "            \"accuracy\": [0.33, 0.50, 0.59, 0.65, 0.70, 0.74, 0.77, 0.78, 0.79, 0.80]\n",
    "        },\n",
    "        \"dependencias\": {\"torch\", \"torchvision\", \"numpy\"},\n",
    "        \"gpu_usado\": \"NVIDIA_V100\",\n",
    "        \"tiempo_total_minutos\": 120\n",
    "    },\n",
    "    \"exp_20241102_001\": {\n",
    "        \"modelo\": \"MobileNetV2\",\n",
    "        \"dataset\": \"ImageNet\",\n",
    "        \"metadata\": (224, 224, 3, 1000),\n",
    "        \"hiperparametros\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"epochs\": 150\n",
    "        },\n",
    "        \"metricas_entrenamiento\": {\n",
    "            \"loss\": [2.5, 2.0, 1.7, 1.4, 1.2, 1.0, 0.92, 0.86, 0.82, 0.79],\n",
    "            \"accuracy\": [0.40, 0.52, 0.60, 0.67, 0.72, 0.76, 0.79, 0.81, 0.83, 0.84]\n",
    "        },\n",
    "        \"metricas_validacion\": {\n",
    "            \"loss\": [2.6, 2.1, 1.8, 1.5, 1.3, 1.1, 1.0, 0.95, 0.92, 0.90],\n",
    "            \"accuracy\": [0.38, 0.50, 0.58, 0.65, 0.70, 0.74, 0.77, 0.79, 0.80, 0.81]\n",
    "        },\n",
    "        \"dependencias\": {\"torch\", \"torchvision\", \"numpy\", \"pillow\", \"opencv\"},\n",
    "        \"gpu_usado\": \"NVIDIA_A100\",\n",
    "        \"tiempo_total_minutos\": 380\n",
    "    },\n",
    "    \"exp_20241102_002\": {\n",
    "        \"modelo\": \"ResNet50\",\n",
    "        \"dataset\": \"CIFAR100\",\n",
    "        \"metadata\": (32, 32, 3, 100),\n",
    "        \"hiperparametros\": {\n",
    "            \"learning_rate\": 0.0001,\n",
    "            \"batch_size\": 32,\n",
    "            \"optimizer\": \"RMSprop\",\n",
    "            \"epochs\": 200\n",
    "        },\n",
    "        \"metricas_entrenamiento\": {\n",
    "            \"loss\": [3.2, 2.5, 2.0, 1.7, 1.5, 1.3, 1.2, 1.1, 1.05, 1.0],\n",
    "            \"accuracy\": [0.25, 0.38, 0.48, 0.55, 0.60, 0.64, 0.67, 0.70, 0.72, 0.74]\n",
    "        },\n",
    "        \"metricas_validacion\": {\n",
    "            \"loss\": [3.3, 2.6, 2.1, 1.8, 1.6, 1.5, 1.4, 1.35, 1.32, 1.30],\n",
    "            \"accuracy\": [0.23, 0.36, 0.46, 0.53, 0.58, 0.61, 0.63, 0.65, 0.66, 0.67]\n",
    "        },\n",
    "        \"dependencias\": {\"torch\", \"torchvision\", \"numpy\", \"albumentations\"},\n",
    "        \"gpu_usado\": \"NVIDIA_V100\",\n",
    "        \"tiempo_total_minutos\": 420\n",
    "    },\n",
    "    \"exp_20241103_001\": {\n",
    "        \"modelo\": \"EfficientNetB0\",\n",
    "        \"dataset\": \"ImageNet\",\n",
    "        \"metadata\": (224, 224, 3, 1000),\n",
    "        \"hiperparametros\": {\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"batch_size\": 64,\n",
    "            \"optimizer\": \"SGD\",\n",
    "            \"epochs\": 120\n",
    "        },\n",
    "        \"metricas_entrenamiento\": {\n",
    "            \"loss\": [2.2, 1.7, 1.4, 1.1, 0.95, 0.83, 0.75, 0.70, 0.66, 0.63],\n",
    "            \"accuracy\": [0.48, 0.60, 0.68, 0.74, 0.78, 0.81, 0.84, 0.86, 0.87, 0.88]\n",
    "        },\n",
    "        \"metricas_validacion\": {\n",
    "            \"loss\": [2.3, 1.8, 1.5, 1.2, 1.0, 0.90, 0.83, 0.79, 0.77, 0.75],\n",
    "            \"accuracy\": [0.46, 0.58, 0.66, 0.72, 0.76, 0.79, 0.82, 0.84, 0.85, 0.86]\n",
    "        },\n",
    "        \"dependencias\": {\"torch\", \"torchvision\", \"numpy\", \"pillow\"},\n",
    "        \"gpu_usado\": \"NVIDIA_A100\",\n",
    "        \"tiempo_total_minutos\": 290\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b00e39",
   "metadata": {},
   "source": [
    "## Parte 1: Análisis de Convergencia y Estabilidad (LISTAS - 25%)\n",
    "\n",
    "Implementa un sistema de análisis de métricas que detecte patrones de entrenamiento problemáticos.\n",
    "\n",
    "**Funciones requeridas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824b2807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'overfitting_detectado': False, 'epoca_inicio': None, 'diferencia_loss_promedio': 0.0, 'epocas_afectadas': []}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'overfitting_detectado': False, 'epoca_inicio': None, 'diferencia_loss_promedio': 0.0, 'epocas_afectadas': []}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'overfitting_detectado': False, 'epoca_inicio': None, 'diferencia_loss_promedio': 0.0, 'epocas_afectadas': []}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'overfitting_detectado': False, 'epoca_inicio': None, 'diferencia_loss_promedio': 0.0, 'epocas_afectadas': []}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'overfitting_detectado': False, 'epoca_inicio': None, 'diferencia_loss_promedio': 0.0, 'epocas_afectadas': []}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def detectar_overfitting_temprano(train_metrics: dict, val_metrics: dict, ventana: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Detecta overfitting analizando la divergencia entre training y validation loss.\n",
    "\n",
    "    Overfitting temprano ocurre cuando:\n",
    "    - El training loss disminuye consistentemente\n",
    "    - El validation loss se estanca o aumenta\n",
    "    - Esta condición se mantiene por 'ventana' épocas consecutivas\n",
    "\n",
    "    Args:\n",
    "        train_metrics: {\"loss\": [...], \"accuracy\": [...]}\n",
    "        val_metrics: {\"loss\": [...], \"accuracy\": [...]}\n",
    "        ventana: Número de épocas consecutivas para confirmar patrón\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"overfitting_detectado\": bool,\n",
    "            \"epoca_inicio\": int o None,\n",
    "            \"diferencia_loss_promedio\": float,\n",
    "            \"epocas_afectadas\": list de índices\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = train_metrics[\"loss\"] # [2.2, 1.7, 1.4, 1.1, 0.95, 0.83, 0.75, 0.70, 0.66, 0.63]\n",
    "    val_loss = val_metrics[\"loss\"]     # [2.3, 1.8, 1.5, 1.2, 1.0, 0.90, 0.83, 0.79, 0.77, 0.75]\n",
    "    epocas_afectadas = []\n",
    "    diferencia_loss_promedio = []\n",
    "    overfitting_detectado = False\n",
    "    consecutivas = 0\n",
    "\n",
    "    for i in range(1, len(train_loss)):\n",
    "\n",
    "        esta_disminuyendo = train_loss[i] < train_loss[i - 1] # Ejemplo i = 1: 1.7 < 2.2 = True\n",
    "\n",
    "        esta_estancado_o_aumentando = val_loss[i] >= val_loss[i-1] # Ejemplo i = 1: 1.8 >= 2.3 = False\n",
    "\n",
    "        # Si ambas condiciones se cumplen, la época tiene el patrón\n",
    "        if esta_disminuyendo and esta_estancado_o_aumentando:\n",
    "            epocas_afectadas.append(i)\n",
    "            consecutivas += 1\n",
    "        else:\n",
    "            epocas_afectadas = []\n",
    "            consecutivas = 0\n",
    "\n",
    "        if consecutivas >= ventana:\n",
    "            overfitting_detectado = True\n",
    "            epoca_inicio = epocas_afectadas[0]\n",
    "            diferencias = [val_loss[i] - train_loss[i] for i in epocas_afectadas] # Ejemplo i = 1: 1.8 - 1.7 = 0.1\n",
    "            diferencia_loss_promedio = sum(diferencias) / len(diferencias)\n",
    "\n",
    "            return{\n",
    "                \"overfitting_detectado\": overfitting_detectado,\n",
    "                \"epoca_inicio\": epoca_inicio,\n",
    "                \"diferencia_loss_promedio\": diferencia_loss_promedio,\n",
    "                \"epocas_afectadas\": epocas_afectadas\n",
    "            }\n",
    "\n",
    "    # Si no detecta overfitting\n",
    "    return{\n",
    "        \"overfitting_detectado\": False,\n",
    "            \"epoca_inicio\": None,\n",
    "            \"diferencia_loss_promedio\": 0.0,\n",
    "            \"epocas_afectadas\": []\n",
    "    }\n",
    "    pass\n",
    "\n",
    "# Casos de uso\n",
    "for exp_id, exp_data in experimentos_db.items():\n",
    "    train_metrics = exp_data[\"metricas_entrenamiento\"]\n",
    "    val_metrics = exp_data[\"metricas_validacion\"]\n",
    "\n",
    "    overfitting_result = detectar_overfitting_temprano(train_metrics, val_metrics)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" Experimento: {exp_id}\")\n",
    "    print(f\" Modelo: {exp_data['modelo']}\")\n",
    "    print(f\" Dataset: {exp_data['dataset']}\\n\")\n",
    "    \n",
    "    print(overfitting_result)\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d15223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "Tasa de convergencia lineal: -0.1722222222222222\n",
      "Tasa de congergencia exponencial: 1.7049607375049916\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "Tasa de convergencia lineal: -0.1711111111111111\n",
      "Tasa de congergencia exponencial: 1.837817680770939\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "Tasa de convergencia lineal: -0.19\n",
      "Tasa de congergencia exponencial: 1.6678030569493767\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "Tasa de convergencia lineal: -0.24444444444444446\n",
      "Tasa de congergencia exponencial: 1.6476216457834105\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "Tasa de convergencia lineal: -0.17444444444444449\n",
      "Tasa de congergencia exponencial: 1.737903295417563\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def calcular_tasa_convergencia(losses: list, metodo: str = \"exponencial\") -> float:\n",
    "    \"\"\"\n",
    "    Calcula la tasa de convergencia del entrenamiento.\n",
    "\n",
    "    Métodos:\n",
    "    - \"lineal\": pendiente promedio entre épocas consecutivas\n",
    "    - \"exponencial\": ajuste a decay exponencial\n",
    "\n",
    "    Una tasa más negativa indica convergencia más rápida.\n",
    "\n",
    "    Args:\n",
    "        losses: Lista de valores de loss por época\n",
    "        metodo: \"lineal\" o \"exponencial\"\n",
    "\n",
    "    Returns:\n",
    "        Tasa de convergencia (valor negativo indica mejora)\n",
    "    \"\"\"\n",
    "    if metodo == \"lineal\":\n",
    "        # Ejemplo: si loss[0]=2.3 y loss[1]=1.8, entonces cambio_0 = 1.8 - 2.3 = -0.5\n",
    "        cambios = [losses[i+1] - losses[i] for i in range(len(losses)-1)]\n",
    "        tasa_convergencia = sum(cambios) / len(cambios)\n",
    "\n",
    "        # Si el resultado es negativo → el modelo está mejorando (loss disminuye)\n",
    "        return tasa_convergencia\n",
    "\n",
    "    else:\n",
    "        ratios = [losses[+1] / losses[i] for i in range(len(losses)-1) if losses[i] != 0]\n",
    "        tasa_exponencial = sum(ratios) / len(ratios)\n",
    "\n",
    "        # Si el resultado es < 1.0 → el modelo está mejorando\n",
    "        return tasa_exponencial\n",
    "            \n",
    "    pass\n",
    "\n",
    "for exp_id, exp_data in experimentos_db.items():\n",
    "    train_losses = exp_data[\"metricas_entrenamiento\"][\"loss\"]\n",
    "\n",
    "    tasa_convergencia_lineal = calcular_tasa_convergencia(train_losses, metodo=\"lineal\")\n",
    "    tasa_convergencia_exponencial = calcular_tasa_convergencia(train_losses, metodo=\"exponencial\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" Experimento: {exp_id}\")\n",
    "    print(f\" Modelo: {exp_data['modelo']}\")\n",
    "    print(f\" Dataset: {exp_data['dataset']}\\n\")\n",
    "\n",
    "    print(f\"Tasa de convergencia lineal: {tasa_convergencia_lineal}\")\n",
    "    print(f\"Tasa de congergencia exponencial: {tasa_convergencia_exponencial}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e85d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      " Se han encontrado: 0 épocas críticas\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      " Se han encontrado: 0 épocas críticas\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      " Se han encontrado: 0 épocas críticas\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      " Se han encontrado: 0 épocas críticas\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      " Se han encontrado: 0 épocas críticas\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def identificar_epocas_criticas(train_losses: list, val_losses: list, threshold: float = 0.05) -> list:\n",
    "    \"\"\"\n",
    "    Identifica épocas donde el modelo muestra comportamiento inestable.\n",
    "\n",
    "    Una época es crítica si:\n",
    "    - El loss aumenta respecto a la época anterior en más de threshold\n",
    "    - Tanto en training como en validation\n",
    "\n",
    "    Args:\n",
    "        train_losses: Losses de entrenamiento\n",
    "        val_losses: Losses de validación\n",
    "        threshold: Umbral de aumento relativo (5% por defecto)\n",
    "\n",
    "    Returns:\n",
    "        Lista de tuplas (epoca_idx, aumento_train, aumento_val)\n",
    "    \"\"\"\n",
    "    epocas_criticas = []\n",
    "\n",
    "    for i in range(1, len(train_losses)):\n",
    "\n",
    "        # [2.3, 1.8, 1.5, 1.2, 1.0, 0.90, 0.83, 0.79, 0.77, 0.75]\n",
    "        aumento_train = (train_losses[i] - train_losses[i-1]) / train_losses[i-1] # (1.8 - 2.3) / 2.3 = -0.217\n",
    "        aumento_val = (val_losses[i] - val_losses[i-1]) / val_losses[i-1]\n",
    "\n",
    "        if (aumento_train > threshold and aumento_val > threshold): # Si ambos aumentos son superiores a 0.05 la época es crítica\n",
    "            epocas_criticas.append((i, aumento_train, aumento_val)) # Hcemos Tupla para almacenar 3 datos relacionados sobre una misma época\n",
    "\n",
    "        return epocas_criticas\n",
    "    pass\n",
    "\n",
    "for exp_id, exp_data in experimentos_db.items():\n",
    "    train_losses = exp_data[\"metricas_entrenamiento\"][\"loss\"]\n",
    "    val_losses = exp_data[\"metricas_validacion\"][\"loss\"]\n",
    "\n",
    "    epocas_criticas = identificar_epocas_criticas(train_losses, val_losses)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" Experimento: {exp_id}\")\n",
    "    print(f\" Modelo: {exp_data['modelo']}\")\n",
    "    print(f\" Dataset: {exp_data['dataset']}\\n\")\n",
    "\n",
    "    print(f\" Se han encontrado: {len(epocas_criticas)} épocas críticas\\n\")\n",
    "\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb967a7",
   "metadata": {},
   "source": [
    "## Parte 2: Gestión de Metadatos y Arquitecturas (TUPLAS - 20%)\n",
    "\n",
    "Trabaja con los metadatos inmutables de las arquitecturas de red.\n",
    "\n",
    "**Funciones requeridas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "413b752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'input_size': 150528, 'output_size': 1000, 'aspect_ratio': 1.0, 'complejidad_relativa': 'media'}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'input_size': 3072, 'output_size': 10, 'aspect_ratio': 1.0, 'complejidad_relativa': 'baja'}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'input_size': 150528, 'output_size': 1000, 'aspect_ratio': 1.0, 'complejidad_relativa': 'media'}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'input_size': 3072, 'output_size': 100, 'aspect_ratio': 1.0, 'complejidad_relativa': 'baja'}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'input_size': 150528, 'output_size': 1000, 'aspect_ratio': 1.0, 'complejidad_relativa': 'media'}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def calcular_complejidad_modelo(metadata: tuple) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula métricas de complejidad basadas en dimensiones de entrada.\n",
    "\n",
    "    metadata formato: (height, width, channels, num_classes)\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"input_size\": int (height * width * channels),\n",
    "            \"output_size\": int (num_classes),\n",
    "            \"aspect_ratio\": float (height / width),\n",
    "            \"complejidad_relativa\": str (\"baja\", \"media\", \"alta\")\n",
    "        }\n",
    "\n",
    "    Complejidad relativa:\n",
    "    - baja: input_size < 50000\n",
    "    - media: 50000 <= input_size < 200000\n",
    "    - alta: input_size >= 200000\n",
    "    \"\"\"\n",
    "\n",
    "    height, width, channels, num_classes = metadata # Desempquetamos la Tupla\n",
    "\n",
    "    input_size = int(height * width * channels)\n",
    "\n",
    "    if input_size < 50000:\n",
    "        complejidad = \"baja\"\n",
    "    elif 50000 <= input_size < 200000:\n",
    "        complejidad = \"media\"\n",
    "    else: # input_size >= 200000\n",
    "        complejidad = \"alta\"\n",
    "\n",
    "    return {\n",
    "        \"input_size\": int (height * width * channels),\n",
    "        \"output_size\": int (num_classes),\n",
    "        \"aspect_ratio\": float (height / width),\n",
    "        \"complejidad_relativa\": str(complejidad)\n",
    "    }\n",
    "    pass\n",
    "\n",
    "for exp_id, exp_data in experimentos_db.items():\n",
    "    metadata = exp_data[\"metadata\"]\n",
    "\n",
    "    complejidad_modelo = calcular_complejidad_modelo(metadata)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" Experimento: {exp_id}\")\n",
    "    print(f\" Modelo: {exp_data['modelo']}\")\n",
    "    print(f\" Dataset: {exp_data['dataset']}\\n\")\n",
    "\n",
    "    print(complejidad_modelo)\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e1506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "('exp_20241101_001', 'ResNet50', 150528, 1000)\n",
      "======================================================================\n",
      "======================================================================\n",
      "('exp_20241102_001', 'MobileNetV2', 150528, 1000)\n",
      "======================================================================\n",
      "======================================================================\n",
      "('exp_20241103_001', 'EfficientNetB0', 150528, 1000)\n",
      "======================================================================\n",
      "======================================================================\n",
      "('exp_20241101_002', 'VGG16', 3072, 10)\n",
      "======================================================================\n",
      "======================================================================\n",
      "('exp_20241102_002', 'ResNet50', 3072, 100)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def comparar_arquitecturas(exp_ids: list, experimentos_db: dict) -> list:\n",
    "    \"\"\"\n",
    "    Compara arquitecturas de múltiples experimentos.\n",
    "\n",
    "    Returns:\n",
    "        Lista de tuplas ordenadas por complejidad descendente:\n",
    "        [(exp_id, modelo, input_size, num_classes), ...]\n",
    "    \"\"\"\n",
    "    lista_tuplas = []\n",
    "\n",
    "    for exp_id in exp_ids:\n",
    "        exp = experimentos_db[exp_id]\n",
    "        modelo = exp[\"modelo\"]\n",
    "        metadata = exp[\"metadata\"]\n",
    "\n",
    "        height, width, channels, num_classes = metadata\n",
    "        input_size = height * width * channels\n",
    "\n",
    "        info_tuplas = (exp_id, modelo, input_size, num_classes)\n",
    "        lista_tuplas.append(info_tuplas)\n",
    "\n",
    "    # x[2] es el input_size (ordena por el tercer elemento de la tupla)\n",
    "    lista_tuplas_ordenadas = sorted(lista_tuplas, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return lista_tuplas_ordenadas\n",
    "    pass\n",
    "\n",
    "arquitecturas_comparadas = comparar_arquitecturas(list(experimentos_db.keys()), experimentos_db)\n",
    "\n",
    "for item in arquitecturas_comparadas:\n",
    "    print(\"=\" * 70)\n",
    "    print(item)\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479bf2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "Vuelta número: 1\n",
      "**********************************************************************\n",
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 990}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 900}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "**********************************************************************\n",
      "Vuelta número: 2\n",
      "**********************************************************************\n",
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 990}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 990}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': True, 'diferencia_clases': 90}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 990}\n",
      "======================================================================\n",
      "**********************************************************************\n",
      "Vuelta número: 3\n",
      "**********************************************************************\n",
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 990}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 900}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "**********************************************************************\n",
      "Vuelta número: 4\n",
      "**********************************************************************\n",
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 900}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': True, 'diferencia_clases': 90}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 900}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 900}\n",
      "======================================================================\n",
      "**********************************************************************\n",
      "Vuelta número: 5\n",
      "**********************************************************************\n",
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 990}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'compatible': False, 'requiere_ajuste_head': True, 'diferencia_clases': 900}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'compatible': True, 'requiere_ajuste_head': False, 'diferencia_clases': 0}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def validar_compatibilidad_transfer_learning(metadata_source: tuple, metadata_target: tuple) -> dict:\n",
    "    \"\"\"\n",
    "    Valida si dos arquitecturas son compatibles para transfer learning.\n",
    "\n",
    "    Compatibilidad requiere:\n",
    "    - Misma resolución (height, width)\n",
    "    - Mismo número de canales\n",
    "    - Puede diferir en num_classes\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"compatible\": bool,\n",
    "            \"requiere_ajuste_head\": bool,\n",
    "            \"diferencia_clases\": int\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    h1, w1, c1, nc1 = metadata_source\n",
    "    h2, w2, c2, nc2 = metadata_target\n",
    "\n",
    "    # 1. Misma resolución:\n",
    "    misma_resolucion = (h1 == h2) and (w1 == w2)\n",
    "\n",
    "    # 2. Mismo número de canales:\n",
    "    mismos_canales = (c1 == c2)\n",
    "\n",
    "    # 3. Compatible si se cumplen 1 y 2\n",
    "    compatible = misma_resolucion and mismos_canales\n",
    "\n",
    "    # 4. Requiere ajuste de cabecera si num_classes difiere:\n",
    "    requiere_ajuste_head = (nc1 != nc2)\n",
    "\n",
    "    # 5. Diferencia en número de clases:\n",
    "    diferencia_clases = abs(nc1 - nc2)\n",
    "\n",
    "    return {\n",
    "        \"compatible\": compatible,\n",
    "        \"requiere_ajuste_head\": requiere_ajuste_head,\n",
    "        \"diferencia_clases\": diferencia_clases\n",
    "    }\n",
    "\n",
    "    pass\n",
    "contador = 0\n",
    "for exp_id, exp_data in experimentos_db.items():\n",
    "    contador += 1\n",
    "    print(\"*\" * 70)\n",
    "    print(f\"Vuelta número: {contador}\")\n",
    "    print(\"*\" * 70)\n",
    "    \n",
    "    metadata_source = exp_data[\"metadata\"]\n",
    "\n",
    "    for exp_id, exp_data in experimentos_db.items():\n",
    "        metadata_target = exp_data[\"metadata\"]\n",
    "\n",
    "        complejidad_transfer_learning = validar_compatibilidad_transfer_learning(metadata_source, metadata_target)\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(f\" Experimento: {exp_id}\")\n",
    "        print(f\" Modelo: {exp_data['modelo']}\")\n",
    "        print(f\" Dataset: {exp_data['dataset']}\\n\")\n",
    "\n",
    "        print(complejidad_transfer_learning)\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024db58b",
   "metadata": {},
   "source": [
    "## Parte 3: Optimización de Configuraciones (DICCIONARIOS - 30%)\n",
    "\n",
    "Analiza y optimiza configuraciones de hiperparámetros basándote en resultados históricos.\n",
    "\n",
    "**Funciones requeridas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2208bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_experimentos(experimentos_db: dict, criterio: str = \"accuracy_final\") -> list:\n",
    "    \"\"\"\n",
    "    Genera ranking de experimentos según diferentes criterios.\n",
    "\n",
    "    Criterios disponibles:\n",
    "    - \"accuracy_final\": Mayor accuracy de validación en última época\n",
    "    - \"convergencia\": Menor número de épocas para alcanzar 80% accuracy val\n",
    "    - \"eficiencia\": Mejor ratio accuracy_final / tiempo_total\n",
    "    - \"estabilidad\": Menor varianza en validation loss últimas 5 épocas\n",
    "\n",
    "    Returns:\n",
    "        Lista de tuplas: [(exp_id, valor_metrica), ...] ordenada descendente\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4123fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mejores_hiperparametros_por_dataset(experimentos_db: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Identifica las mejores configuraciones de hiperparámetros por dataset.\n",
    "\n",
    "    Para cada dataset, encuentra el experimento con mejor accuracy final\n",
    "    y extrae sus hiperparámetros.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"dataset_name\": {\n",
    "                \"mejor_exp_id\": str,\n",
    "                \"hiperparametros\": dict,\n",
    "                \"accuracy_alcanzada\": float\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0266bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_impacto_hiperparametros(experimentos_db: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Analiza el impacto de cada hiperparámetro en el rendimiento.\n",
    "\n",
    "    Agrupa experimentos por valor de hiperparámetro y calcula\n",
    "    accuracy promedio para cada valor único.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"learning_rate\": {\n",
    "                0.001: {\"avg_accuracy\": X, \"num_experimentos\": N},\n",
    "                0.01: {\"avg_accuracy\": Y, \"num_experimentos\": M},\n",
    "                ...\n",
    "            },\n",
    "            \"batch_size\": {...},\n",
    "            \"optimizer\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2f2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_configuracion_optima(experimentos_db: dict, dataset: str, modelo: str) -> dict:\n",
    "    \"\"\"\n",
    "    Genera configuración óptima para un par dataset-modelo basándose en histórico.\n",
    "\n",
    "    Si existe experimento exacto con ese dataset y modelo:\n",
    "        - Usa esos hiperparámetros\n",
    "    Si no:\n",
    "        - Usa hiperparámetros más frecuentes en experimentos del dataset\n",
    "        - Si no hay del dataset, usa promedios globales\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"hiperparametros_sugeridos\": dict,\n",
    "            \"confianza\": str (\"alta\", \"media\", \"baja\"),\n",
    "            \"experimentos_referencia\": list de exp_ids usados\n",
    "        }\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acab19",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## Parte 4: Gestión de Dependencias y Recursos (SETS - 25%)\n",
    "\n",
    "Valida compatibilidad de entornos y optimiza el uso de recursos computacionales.\n",
    "\n",
    "**Funciones requeridas:**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6191284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'universales': {'numpy', 'torch', 'torchvision'}, 'mayoritarias': {'pillow', 'numpy', 'torch', 'torchvision'}, 'opcionales': {'opencv', 'albumentations'}, 'por_modelo': {'ResNet50': {'pillow', 'numpy', 'torch', 'torchvision', 'albumentations'}, 'VGG16': {'numpy', 'torch', 'torchvision'}, 'MobileNetV2': {'pillow', 'numpy', 'torch', 'torchvision', 'opencv'}, 'EfficientNetB0': {'pillow', 'numpy', 'torch', 'torchvision'}}}\n"
     ]
    }
   ],
   "source": [
    "def dependencias_comunes(experimentos_db: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Analiza las dependencias de todos los experimentos.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"universales\": set,  # Presentes en TODOS los experimentos\n",
    "            \"mayoritarias\": set,  # Presentes en >50% experimentos\n",
    "            \"opcionales\": set,   # Presentes en <50% experimentos\n",
    "            \"por_modelo\": {\n",
    "                \"ResNet50\": set,\n",
    "                \"VGG16\": set,\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener la lista de todos los sets de dependencias:\n",
    "    todos_deps = [exp[\"dependencias\"] for exp in experimentos_db.values()]\n",
    "\n",
    "    # Primera iteración: comenzar con el primer set\n",
    "    universales = todos_deps[0]\n",
    "\n",
    "    # Interactuar con cada uno de los demás\n",
    "    for deps in todos_deps[1:]:\n",
    "        universales = universales & deps # & Operador de intersección\n",
    "\n",
    "    total_experimentos = len(experimentos_db)\n",
    "    umbral = total_experimentos / 2\n",
    "\n",
    "    # Unión de todas las dependencias (vocabulario completo):\n",
    "    todas_dependencias = set()\n",
    "    for exp in experimentos_db.values():\n",
    "        todas_dependencias = todas_dependencias | exp[\"dependencias\"] # | Operador de unión\n",
    "\n",
    "    # Contar en cuántos experimentos aparece cada dependencia:\n",
    "    mayoritarias = set()\n",
    "    for dep in todas_dependencias:\n",
    "        contador = 0\n",
    "        for exp in experimentos_db.values():\n",
    "            if dep in exp[\"dependencias\"]:\n",
    "                contador += 1\n",
    "        if contador > umbral:\n",
    "            mayoritarias.add(dep)\n",
    "\n",
    "    # Todas las dependencias que no son opcionales\n",
    "    opcionales = todas_dependencias - mayoritarias # Operador de difference\n",
    "\n",
    "    por_modelo = {}\n",
    "    for exp_id, exp_data in experimentos_db.items():\n",
    "        nombre_modelo = exp_data[\"modelo\"]\n",
    "        if nombre_modelo not in por_modelo:\n",
    "            por_modelo[nombre_modelo] = set()\n",
    "        por_modelo[nombre_modelo] |= exp_data[\"dependencias\"]\n",
    "\n",
    "    return {\n",
    "            \"universales\": universales,  # Presentes en TODOS los experimentos\n",
    "            \"mayoritarias\": mayoritarias,  # Presentes en >50% experimentos\n",
    "            \"opcionales\": opcionales,   # Presentes en <50% experimentos\n",
    "            \"por_modelo\": por_modelo\n",
    "        }\n",
    "    pass\n",
    "\n",
    "dependencias_common = dependencias_comunes(experimentos_db)\n",
    "print(dependencias_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f45b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " Experimento: exp_20241101_001\n",
      " Modelo: ResNet50\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'puede_ejecutar': False, 'dependencias_faltantes': {'pillow', 'torchvision'}, 'dependencias_adicionales': {'matplotlib', 'pandas'}, 'recomendaciones': [\"Instalar: {'pillow', 'torchvision'}\"]}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241101_002\n",
      " Modelo: VGG16\n",
      " Dataset: CIFAR10\n",
      "\n",
      "{'puede_ejecutar': False, 'dependencias_faltantes': {'torchvision'}, 'dependencias_adicionales': {'matplotlib', 'pandas'}, 'recomendaciones': [\"Instalar: {'torchvision'}\"]}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_001\n",
      " Modelo: MobileNetV2\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'puede_ejecutar': False, 'dependencias_faltantes': {'pillow', 'opencv', 'torchvision'}, 'dependencias_adicionales': {'matplotlib', 'pandas'}, 'recomendaciones': [\"Instalar: {'pillow', 'opencv', 'torchvision'}\"]}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241102_002\n",
      " Modelo: ResNet50\n",
      " Dataset: CIFAR100\n",
      "\n",
      "{'puede_ejecutar': False, 'dependencias_faltantes': {'albumentations', 'torchvision'}, 'dependencias_adicionales': {'matplotlib', 'pandas'}, 'recomendaciones': [\"Instalar: {'albumentations', 'torchvision'}\"]}\n",
      "======================================================================\n",
      "======================================================================\n",
      " Experimento: exp_20241103_001\n",
      " Modelo: EfficientNetB0\n",
      " Dataset: ImageNet\n",
      "\n",
      "{'puede_ejecutar': False, 'dependencias_faltantes': {'pillow', 'torchvision'}, 'dependencias_adicionales': {'matplotlib', 'pandas'}, 'recomendaciones': [\"Instalar: {'pillow', 'torchvision'}\"]}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def validar_entorno(dependencias_disponibles: set, exp_id: str, experimentos_db: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Valida si un entorno tiene las dependencias necesarias para un experimento.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"puede_ejecutar\": bool,\n",
    "            \"dependencias_faltantes\": set,\n",
    "            \"dependencias_adicionales\": set,  # Están disponibles pero no necesarias\n",
    "            \"recomendaciones\": list de str\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    deps_necesarias = experimentos_db[exp_id][\"dependencias\"]\n",
    "    deps_disponibles = dependencias_disponibles # Set proporcionado\n",
    "\n",
    "    # 1. Puede ejecutar si tiene todas las dependencias necesarias:\n",
    "    puede_ejecutar = deps_necesarias.issubset(deps_disponibles)\n",
    "\n",
    "    # 2. Dependencias faltantes:\n",
    "    deps_faltantes = deps_necesarias - deps_disponibles\n",
    "\n",
    "    # 3. Dependencias adicionales (disponibles pero no necesarias):\n",
    "    deps_adicionales = deps_disponibles - deps_necesarias\n",
    "\n",
    "    # 4. Recomendaciones:\n",
    "    recomendaciones = []\n",
    "    if len(deps_faltantes) > 0:\n",
    "        recomendaciones.append(f\"Instalar: {deps_faltantes}\")\n",
    "    if len(deps_adicionales) > 5:\n",
    "        recomendaciones.append(\"Considerar limpiar entorno, hay muchas dependencias no usadas\")\n",
    "\n",
    "    return {\n",
    "        \"puede_ejecutar\": puede_ejecutar,\n",
    "        \"dependencias_faltantes\": deps_faltantes,\n",
    "        \"dependencias_adicionales\": deps_adicionales,  # Están disponibles pero no necesarias\n",
    "        \"recomendaciones\": recomendaciones\n",
    "    }\n",
    "\n",
    "    pass\n",
    "\n",
    "deps_disponibles = {\"torch\", \"numpy\", \"pandas\", \"matplotlib\"}\n",
    "\n",
    "for exp_id, exp_data in experimentos_db.items():\n",
    "    metadata = exp_data[\"metadata\"]\n",
    "\n",
    "    validacion_entorno = validar_entorno(deps_disponibles, exp_id, experimentos_db)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" Experimento: {exp_id}\")\n",
    "    print(f\" Modelo: {exp_data['modelo']}\")\n",
    "    print(f\" Dataset: {exp_data['dataset']}\\n\")\n",
    "\n",
    "    print(validacion_entorno)\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885f644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " GPU: NVIDIA_A100\n",
      " Tiempo total minutos: 915\n",
      " Número de experimentos: 3\n",
      " Tiempo promedio: 305.0\n",
      " Modelos principales: {'ResNet50', 'EfficientNetB0', 'MobileNetV2'}\n",
      " Porcentaje de utilización 91.5\n",
      "======================================================================\n",
      "======================================================================\n",
      " GPU: NVIDIA_V100\n",
      " Tiempo total minutos: 540\n",
      " Número de experimentos: 2\n",
      " Tiempo promedio: 270.0\n",
      " Modelos principales: {'VGG16', 'ResNet50'}\n",
      " Porcentaje de utilización 54.0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def optimizar_uso_gpu(experimentos_db: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Analiza el uso de GPUs y sugiere optimizaciones.\n",
    "\n",
    "    Calcula por GPU:\n",
    "    - Tiempo total utilizado\n",
    "    - Número de experimentos ejecutados\n",
    "    - Tiempo promedio por experimento\n",
    "    - Modelos más ejecutados en cada GPU\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"NVIDIA_A100\": {\n",
    "                \"tiempo_total_minutos\": int,\n",
    "                \"num_experimentos\": int,\n",
    "                \"tiempo_promedio\": float,\n",
    "                \"modelos_principales\": set,\n",
    "                \"utilizacion_porcentaje\": float  # Asumiendo max 1000 min disponibles\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    gpus_info = {}\n",
    "\n",
    "    for exp in experimentos_db.values():\n",
    "        gpu = exp[\"gpu_usado\"]\n",
    "        tiempo = exp[\"tiempo_total_minutos\"]\n",
    "        modelo = exp[\"modelo\"]\n",
    "\n",
    "        # Inicializar si es la primera vez:\n",
    "        if gpu not in gpus_info:\n",
    "            gpus_info[gpu] = {\n",
    "                \"tiempo_total_minutos\": 0,\n",
    "                \"num_experimentos\": 0,\n",
    "                \"modelos\": []\n",
    "            }\n",
    "\n",
    "        # Acumular:\n",
    "        gpus_info[gpu][\"tiempo_total_minutos\"] += tiempo\n",
    "        gpus_info[gpu][\"num_experimentos\"] += 1\n",
    "        gpus_info[gpu][\"modelos\"].append(modelo)\n",
    "\n",
    "    # Calcular promedios y modelos principales:\n",
    "    for gpu, info in gpus_info.items():\n",
    "        # Tiempo promedio:\n",
    "        tiempo_promedio = info[\"tiempo_total_minutos\"] / info[\"num_experimentos\"]\n",
    "\n",
    "        # Modelos principales (los más frecuentes):\n",
    "        modelos_set = set(info[\"modelos\"])  # Eliminar duplicados\n",
    "\n",
    "        # Utilización (asumiendo 1000 minutos disponibles):\n",
    "        utilizacion = (info[\"tiempo_total_minutos\"] / 1000) * 100\n",
    "\n",
    "        # Actualizar:\n",
    "        gpus_info[gpu][\"tiempo_promedio\"] = tiempo_promedio\n",
    "        gpus_info[gpu][\"modelos_principales\"] = modelos_set\n",
    "        gpus_info[gpu][\"utilizacion_porcentaje\"] = utilizacion\n",
    "\n",
    "    return gpus_info\n",
    "    pass\n",
    "\n",
    "optimizar_gpu = optimizar_uso_gpu(experimentos_db)\n",
    "\n",
    "for gpu, gpu_info in optimizar_gpu.items():\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" GPU: {gpu}\")\n",
    "    print(f\" Tiempo total minutos: {gpu_info[\"tiempo_total_minutos\"]}\")\n",
    "    print(f\" Número de experimentos: {gpu_info[\"num_experimentos\"]}\")\n",
    "    print(f\" Tiempo promedio: {gpu_info[\"tiempo_promedio\"]}\")\n",
    "    print(f\" Modelos principales: {gpu_info[\"modelos_principales\"]}\")\n",
    "    print(f\" Porcentaje de utilización {gpu_info[\"utilizacion_porcentaje\"]}\") # Asumiendo max 1000 min disponibles\n",
    "\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c10f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conflictos_dependencias(experimentos_a_ejecutar: list, experimentos_db: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Detecta posibles conflictos si se ejecutan múltiples experimentos simultáneamente.\n",
    "\n",
    "    Args:\n",
    "        experimentos_a_ejecutar: Lista de exp_ids a ejecutar en paralelo\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"puede_parallelizar\": bool,\n",
    "            \"dependencias_conflictivas\": set,  # Dependencias que no coinciden\n",
    "            \"gpu_compartida\": bool,  # True si requieren misma GPU\n",
    "            \"recomendacion\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577baf0",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## Función Integradora Final (BONUS - 20% extra)\n",
    "\n",
    "Implementa una función que use TODAS las colecciones de datos de forma integrada:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def informe_completo_experimento(exp_id: str, experimentos_db: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Genera un informe completo de un experimento usando todas las colecciones.\n",
    "\n",
    "    El informe debe incluir:\n",
    "\n",
    "    1. Metadata del modelo (TUPLA):\n",
    "       - Dimensiones de entrada\n",
    "       - Complejidad calculada\n",
    "\n",
    "    2. Análisis de convergencia (LISTAS):\n",
    "       - Tasa de convergencia\n",
    "       - Overfitting detectado\n",
    "       - Épocas críticas\n",
    "\n",
    "    3. Contexto comparativo (DICCIONARIOS):\n",
    "       - Posición en ranking general\n",
    "       - Comparación con otros experimentos del mismo dataset\n",
    "       - Comparación con otros experimentos del mismo modelo\n",
    "\n",
    "    4. Validación de entorno (SETS):\n",
    "       - Dependencias únicas de este experimento\n",
    "       - Dependencias compartidas con otros experimentos exitosos\n",
    "       - Compatibilidad con entorno estándar\n",
    "\n",
    "    Returns:\n",
    "        Diccionario estructurado con toda la información analizada\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
