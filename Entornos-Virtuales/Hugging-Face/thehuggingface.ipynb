{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masterclass: Introducción a Hugging Face para NLP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Hugging Face?\n",
    "\n",
    "### 1.1 Contextualicemos\n",
    "\n",
    "Hugging Face es una plataforma que proporciona tres componentes principales:\n",
    "\n",
    "- **Hub de modelos**\n",
    "\n",
    "- **Librería transformers**\n",
    "\n",
    "- **Ecosistema complementario**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación básica con PyTorch\n",
    "# !pip install transformers torch\n",
    "\n",
    "# Instalación con TensorFlow\n",
    "# !pip install transformers tensorflow\n",
    "\n",
    "# Instalación completa con dependencias adicionales\n",
    "# !pip install transformers[torch,sentencepiece,tokenizers]\n",
    "\n",
    "# Para trabajar con audio\n",
    "# !pip install transformers[torch,audio]\n",
    "\n",
    "# Para trabajar con visión\n",
    "# !pip install transformers[torch,vision]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.57.1\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA disponible: False\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configuración de caché\n",
    "\n",
    "Por defecto, los modelos se descargan en `~/.cache/huggingface/hub`. Para cambiar la ubicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Establecer directorio de caché personalizado\n",
    "os.environ['HF_HOME'] = '/ruta/personalizada/cache'\n",
    "\n",
    "# Verificar configuración\n",
    "print(f\"Directorio de caché: {os.environ.get('HF_HOME', '~/.cache/huggingface')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Búsqueda de modelos en Hugging Face\n",
    "\n",
    "### 2.1 Búsqueda manual en el Hub\n",
    "\n",
    "Navegación web: https://huggingface.co/models\n",
    "\n",
    "**Filtros disponibles:**\n",
    "\n",
    "- **Task**: text-classification, token-classification, question-answering, summarization, translation, text-generation, fill-mask, etc.\n",
    "- **Library**: transformers, sentence-transformers, spacy, flair\n",
    "- **Language**: en, es, fr, multilingual, etc.\n",
    "- **License**: apache-2.0, mit, cc-by-sa-4.0, commercial licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Búsqueda programática con Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "# Crear cliente API\n",
    "api = HfApi()\n",
    "\n",
    "# Búsqueda básica: modelos de clasificación de texto\n",
    "models = list_models(\n",
    "    filter=\"text-classification\",\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "print(\"Top 10 modelos de clasificación de texto:\")\n",
    "for model in models:\n",
    "    print(f\"- {model.modelId} | Descargas: {model.downloads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Búsqueda avanzada con múltiples filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar modelos de NER en español\n",
    "models_ner_es = list_models(\n",
    "    filter={\"task\": \"token-classification\", \"language\": \"es\"},\n",
    "    sort=\"downloads\",\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(\"\\nModelos de NER en español:\")\n",
    "for model in models_ner_es:\n",
    "    print(f\"- {model.modelId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar modelos de traducción inglés-español\n",
    "models_translation = list_models(\n",
    "    filter=\"translation\",\n",
    "    search=\"en-es\",\n",
    "    sort=\"likes\",\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(\"\\nModelos de traducción EN-ES:\")\n",
    "for model in models_translation:\n",
    "    print(f\"- {model.modelId} | Likes: {model.likes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Categorías principales de tareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de tareas disponibles\n",
    "TASKS = {\n",
    "    # Texto\n",
    "    \"text-classification\": \"Clasificación de texto (sentimiento, spam, categorías)\",\n",
    "    \"token-classification\": \"Clasificación de tokens (NER, POS tagging)\",\n",
    "    \"question-answering\": \"Respuesta a preguntas dado un contexto\",\n",
    "    \"summarization\": \"Resumen de textos largos\",\n",
    "    \"translation\": \"Traducción entre idiomas\",\n",
    "    \"text-generation\": \"Generación de texto continuado\",\n",
    "    \"fill-mask\": \"Completar texto con máscaras\",\n",
    "    \"text2text-generation\": \"Transformación texto a texto (paráfrasis, etc.)\",\n",
    "    \n",
    "    # Audio\n",
    "    \"automatic-speech-recognition\": \"Transcripción de audio a texto\",\n",
    "    \"audio-classification\": \"Clasificación de audio (género musical, emociones)\",\n",
    "    \"text-to-speech\": \"Síntesis de voz\",\n",
    "    \n",
    "    # Visión\n",
    "    \"image-classification\": \"Clasificación de imágenes\",\n",
    "    \"object-detection\": \"Detección de objetos en imágenes\",\n",
    "    \"image-segmentation\": \"Segmentación de imágenes\",\n",
    "    \n",
    "    # Multimodal\n",
    "    \"visual-question-answering\": \"Responder preguntas sobre imágenes\",\n",
    "    \"image-to-text\": \"Generar descripciones de imágenes\"\n",
    "}\n",
    "\n",
    "for task, description in TASKS.items():\n",
    "    print(f\"{task}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Recomendaciones de modelos por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMMENDED_MODELS = {\n",
    "    # Clasificación de texto - inglés\n",
    "    \"sentiment_en\": [\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",  # Rápido, eficiente\n",
    "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\",  # Especializado en redes sociales\n",
    "    ],\n",
    "    \n",
    "    # Clasificación de texto - español\n",
    "    \"sentiment_es\": [\n",
    "        \"finiteautomata/beto-sentiment-analysis\",  # BETO adaptado\n",
    "        \"pysentimiento/robertuito-sentiment-analysis\",  # Especializado en español latinoamericano\n",
    "    ],\n",
    "    \n",
    "    # NER - inglés\n",
    "    \"ner_en\": [\n",
    "        \"dslim/bert-base-NER\",  # General purpose\n",
    "        \"dbmdz/bert-large-cased-finetuned-conll03-english\",  # Alta precisión\n",
    "    ],\n",
    "    \n",
    "    # NER - español\n",
    "    \"ner_es\": [\n",
    "        \"mrm8488/bert-spanish-cased-finetuned-ner\",\n",
    "        \"dccuchile/bert-base-spanish-wwm-cased-finetuned-conll02-spanish\",\n",
    "    ],\n",
    "    \n",
    "    # Question Answering\n",
    "    \"qa_en\": [\n",
    "        \"distilbert-base-cased-distilled-squad\",  # Eficiente\n",
    "        \"deepset/roberta-base-squad2\",  # Mayor precisión\n",
    "    ],\n",
    "    \n",
    "    # Traducción\n",
    "    \"translation\": [\n",
    "        \"Helsinki-NLP/opus-mt-en-es\",  # EN -> ES\n",
    "        \"Helsinki-NLP/opus-mt-es-en\",  # ES -> EN\n",
    "    ],\n",
    "    \n",
    "    # Summarization\n",
    "    \"summarization\": [\n",
    "        \"facebook/bart-large-cnn\",  # Noticias\n",
    "        \"t5-base\",  # Propósito general\n",
    "    ],\n",
    "    \n",
    "    # Text Generation\n",
    "    \"generation\": [\n",
    "        \"gpt2\",  # Baseline\n",
    "        \"distilgpt2\",  # Más ligero\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Función para obtener recomendaciones\n",
    "def get_recommended_model(task: str, index: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene modelo recomendado para una tarea.\n",
    "    \n",
    "    Args:\n",
    "        task: Clave de la tarea\n",
    "        index: Índice del modelo (0 = más recomendado)\n",
    "    \n",
    "    Returns:\n",
    "        Nombre del modelo\n",
    "    \"\"\"\n",
    "    if task in RECOMMENDED_MODELS:\n",
    "        return RECOMMENDED_MODELS[task][index]\n",
    "    else:\n",
    "        raise ValueError(f\"Tarea {task} no encontrada\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "model_name = get_recommended_model(\"sentiment_en\", 0)\n",
    "print(f\"Modelo recomendado para sentiment analysis: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Información detallada de un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import model_info\n",
    "\n",
    "# Obtener metadatos de un modelo\n",
    "info = model_info(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(f\"Modelo: {info.modelId}\")\n",
    "print(f\"Tarea: {info.pipeline_tag}\")\n",
    "print(f\"Descargas: {info.downloads}\")\n",
    "print(f\"Likes: {info.likes}\")\n",
    "print(f\"Licencia: {info.cardData.get('license', 'No especificada')}\")\n",
    "print(f\"Idiomas: {info.cardData.get('language', 'No especificado')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ¿Qué es un Pipeline?\n",
    "\n",
    "### 3.1 Definición técnica\n",
    "\n",
    "Un **pipeline** es una abstracción de alto nivel que encapsula tres componentes del proceso de inferencia:\n",
    "\n",
    "1. **Preprocessing (Tokenizer)**: Convierte texto crudo en tensores numéricos que el modelo puede procesar\n",
    "2. **Model**: Red neuronal que procesa los tensores y genera representaciones\n",
    "3. **Postprocessing**: Transforma las salidas del modelo en formato interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Arquitectura interna\n",
    "\n",
    "#### Proceso manual (sin pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. Preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "inputs = tokenizer(\"This is great!\", return_tensors=\"pt\")\n",
    "\n",
    "# 2. Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 3. Postprocessing\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "\n",
    "print(f\"Clase predicha: {predicted_class}\")\n",
    "print(f\"Probabilidades: {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mismo proceso usando pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"This is great!\")\n",
    "\n",
    "print(result)\n",
    "# [{'label': 'POSITIVE', 'score': 0.9998}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ventajas del pipeline\n",
    "\n",
    "- **Simplificación**: Una línea de código vs. múltiples pasos\n",
    "- **Configuración automática**: Descarga y configura tokenizer y modelo compatible\n",
    "- **Manejo de errores**: Gestión automática de casos edge (textos vacíos, muy largos, etc.)\n",
    "- **Optimizaciones**: Batch processing, device management (CPU/GPU)\n",
    "\n",
    "### 3.4 Limitaciones del pipeline\n",
    "\n",
    "- **Menos control**: No acceso a representaciones intermedias\n",
    "- **Personalización limitada**: Difícil modificar preprocessing o postprocessing\n",
    "- **Rendimiento**: Overhead en producción de alto volumen\n",
    "\n",
    "**Recomendación**: Usar pipelines para prototipado rápido y demos. Para producción, considerar implementación manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Sintaxis básica para utilizar cualquier modelo\n",
    "\n",
    "### 4.1 Sintaxis con pipeline (alto nivel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sintaxis general\n",
    "pipeline_obj = pipeline(\n",
    "    task=\"nombre-de-tarea\",              # Obligatorio\n",
    "    model=\"nombre-modelo\",                # Opcional (usa modelo default)\n",
    "    tokenizer=\"nombre-tokenizer\",         # Opcional (usa tokenizer del modelo)\n",
    "    device=0,                             # Opcional (0=GPU, -1=CPU)\n",
    "    batch_size=8,                         # Opcional (procesamiento por lotes)\n",
    "    max_length=512,                       # Opcional (longitud máxima)\n",
    "    truncation=True                       # Opcional (truncar si excede max_length)\n",
    ")\n",
    "\n",
    "# Uso\n",
    "# resultado = pipeline_obj(\"texto de entrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sintaxis manual (bajo nivel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. Cargar tokenizer y modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nombre-del-modelo\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nombre-del-modelo\")\n",
    "\n",
    "# 2. Preparar datos\n",
    "text = \"texto de entrada\"\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",      # PyTorch tensors\n",
    "    padding=True,             # Padding a longitud máxima del batch\n",
    "    truncation=True,          # Truncar si excede max_length\n",
    "    max_length=512            # Longitud máxima\n",
    ")\n",
    "\n",
    "# 3. Inferencia\n",
    "model.eval()  # Modo evaluación\n",
    "with torch.no_grad():  # Sin calcular gradientes\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 4. Procesar salida\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Clases Auto* para diferentes tareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,    # Clasificación de texto\n",
    "    AutoModelForTokenClassification,       # NER, POS tagging\n",
    "    AutoModelForQuestionAnswering,         # Question Answering\n",
    "    AutoModelForSeq2SeqLM,                 # Traducción, summarization\n",
    "    AutoModelForCausalLM,                  # Generación de texto (GPT)\n",
    "    AutoModelForMaskedLM                   # Fill-mask (BERT)\n",
    ")\n",
    "\n",
    "# Ejemplo: selección automática según arquitectura\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Las clases Auto* detectan automáticamente la arquitectura\n",
    "model_classification = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model_masked_lm = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Sintaxis para batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "results = classifier(texts)  # Procesa automáticamente en batch\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Gestión de dispositivo (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\frasq\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsando dispositivo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Con pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m classifier = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 0 = primera GPU\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:268\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    266\u001b[39m         classes.append(_class)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     _class = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m         classes.append(_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     TFBaseModelOutput,\n\u001b[32m     29\u001b[39m     TFMaskedLMOutput,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     TFTokenClassifierOutput,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     TFMaskedLanguageModelingLoss,\n\u001b[32m     37\u001b[39m     TFModelInputType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     unpack_inputs,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Verificar disponibilidad de CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Con pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", device=0)  # 0 = primera GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.to(device)\n",
    "\n",
    "inputs = tokenizer(\"texto\", return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover inputs a GPU\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Ejemplos prácticos\n",
    "\n",
    "### 5.1 Ejemplo 1: Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Analizar reviews de productos\n",
    "reviews = [\n",
    "    \"This product exceeded all my expectations. Highly recommended!\",\n",
    "    \"Terrible quality. Broke after one day of use.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Best purchase I've made this year!\"\n",
    "]\n",
    "\n",
    "results = sentiment_analyzer(reviews)\n",
    "\n",
    "# Mostrar resultados\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Amazon procesa millones de reviews diariamente para calcular ratings ajustados por sentimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ejemplo 2: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline de NER\n",
    "ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    grouped_entities=True  # Agrupa tokens de la misma entidad\n",
    ")\n",
    "\n",
    "# Caso de uso: Extraer información de artículos de noticias\n",
    "text = \"\"\"\n",
    "Apple Inc. announced that CEO Tim Cook will visit the new headquarters \n",
    "in Cupertino, California next Monday. The company reported revenue of \n",
    "$394 billion in 2023.\n",
    "\"\"\"\n",
    "\n",
    "entities = ner(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "print(\"Entidades detectadas:\")\n",
    "for entity in entities:\n",
    "    print(f\"- {entity['word']}: {entity['entity_group']} (score: {entity['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Reuters utiliza NER para etiquetar automáticamente noticias y crear bases de datos de eventos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Ejemplo 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Sistema de FAQ automatizado\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications \n",
    "using machine learning. The company was founded in 2016 and is based in \n",
    "New York City. Their main product is the Transformers library, which \n",
    "provides APIs and tools to download and train state-of-the-art pretrained \n",
    "models. The library supports PyTorch, TensorFlow, and JAX.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Where is the company based?\",\n",
    "    \"What frameworks does the library support?\"\n",
    "]\n",
    "\n",
    "# Responder preguntas\n",
    "for question in questions:\n",
    "    result = qa_model(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (score: {result['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Zendesk implementa QA para responder automáticamente tickets de soporte basándose en documentación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Ejemplo 4: Traducción automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline de traducción\n",
    "translator_en_es = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-es\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Traducir documentación técnica\n",
    "english_texts = [\n",
    "    \"Machine learning models require large amounts of data for training.\",\n",
    "    \"The transformer architecture revolutionized natural language processing.\",\n",
    "    \"Python is the most popular programming language for data science.\"\n",
    "]\n",
    "\n",
    "# Traducir\n",
    "translations = translator_en_es(english_texts)\n",
    "\n",
    "# Mostrar resultados\n",
    "for original, translation in zip(english_texts, translations):\n",
    "    print(f\"EN: {original}\")\n",
    "    print(f\"ES: {translation['translation_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Microsoft Translator API utiliza arquitecturas similares para traducción en tiempo real en Teams y Skype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Ejemplo 5: Fill-mask (completar texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline\n",
    "unmasker = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Sugerencias de autocompletado\n",
    "text_with_mask = \"The capital of France is [MASK].\"\n",
    "\n",
    "# Obtener predicciones\n",
    "predictions = unmasker(text_with_mask)\n",
    "\n",
    "# Mostrar top 5 predicciones\n",
    "print(f\"Texto: {text_with_mask}\")\n",
    "print(\"\\nPredicciones:\")\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    print(f\"{i}. {pred['token_str']}: {pred['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Gmail utiliza modelos similares para Smart Compose (sugerencias de autocompletado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Ejemplo 6: Implementación manual con control detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentClassifier:\n",
    "    \"\"\"\n",
    "    Clasificador de sentimiento con control manual del proceso.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Inicializa el clasificador.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Nombre del modelo en Hugging Face\n",
    "            device: 'cpu' o 'cuda'\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Mapeo de etiquetas\n",
    "        self.labels = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    \n",
    "    def predict(self, text: str, return_probs: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Predice el sentimiento de un texto.\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a clasificar\n",
    "            return_probs: Si True, devuelve probabilidades de todas las clases\n",
    "            \n",
    "        Returns:\n",
    "            dict con label, score y opcionalmente probabilities\n",
    "        \"\"\"\n",
    "        # Tokenización\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Mover a dispositivo\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Inferencia\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Predicción\n",
    "        predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0][predicted_class].item()\n",
    "        \n",
    "        result = {\n",
    "            \"label\": self.labels[predicted_class],\n",
    "            \"score\": confidence\n",
    "        }\n",
    "        \n",
    "        if return_probs:\n",
    "            result[\"probabilities\"] = {\n",
    "                label: probs[0][idx].item() \n",
    "                for idx, label in self.labels.items()\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_batch(self, texts: list) -> list:\n",
    "        \"\"\"\n",
    "        Predice el sentimiento de múltiples textos.\n",
    "        \n",
    "        Args:\n",
    "            texts: Lista de textos\n",
    "            \n",
    "        Returns:\n",
    "            Lista de diccionarios con predicciones\n",
    "        \"\"\"\n",
    "        # Tokenización por lotes\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Mover a dispositivo\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Inferencia\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Procesar resultados\n",
    "        results = []\n",
    "        for i in range(len(texts)):\n",
    "            predicted_class = torch.argmax(probs[i]).item()\n",
    "            confidence = probs[i][predicted_class].item()\n",
    "            \n",
    "            results.append({\n",
    "                \"label\": self.labels[predicted_class],\n",
    "                \"score\": confidence\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso del clasificador\n",
    "\n",
    "# Inicializar\n",
    "classifier = SentimentClassifier(\n",
    "    model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Predicción individual\n",
    "result = classifier.predict(\n",
    "    \"This product is amazing!\",\n",
    "    return_probs=True\n",
    ")\n",
    "print(\"Predicción individual:\")\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "# Predicción por lotes\n",
    "texts = [\n",
    "    \"Great service and fast delivery!\",\n",
    "    \"Disappointed with the quality.\",\n",
    "    \"Average product, nothing special.\"\n",
    "]\n",
    "\n",
    "results = classifier.predict_batch(texts)\n",
    "print(\"Predicción por lotes:\")\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"{text} -> {result['label']} ({result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Ejemplo 7: Manejo de errores y casos edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def safe_sentiment_analysis(texts: list) -> list:\n",
    "    \"\"\"\n",
    "    Análisis de sentimiento con manejo de errores.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos a analizar\n",
    "        \n",
    "    Returns:\n",
    "        Lista de resultados con manejo de errores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        classifier = pipeline(\"sentiment-analysis\")\n",
    "        results = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                # Validar entrada\n",
    "                if not isinstance(text, str):\n",
    "                    logger.warning(f\"Input no es string: {type(text)}\")\n",
    "                    results.append({\"error\": \"Invalid input type\"})\n",
    "                    continue\n",
    "                \n",
    "                if len(text.strip()) == 0:\n",
    "                    logger.warning(\"Texto vacío\")\n",
    "                    results.append({\"error\": \"Empty text\"})\n",
    "                    continue\n",
    "                \n",
    "                # Truncar si es muy largo\n",
    "                if len(text) > 10000:\n",
    "                    logger.warning(f\"Texto muy largo ({len(text)} caracteres), truncando\")\n",
    "                    text = text[:10000]\n",
    "                \n",
    "                # Predecir\n",
    "                result = classifier(text)[0]\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error procesando texto: {str(e)}\")\n",
    "                results.append({\"error\": str(e)})\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inicializando pipeline: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Prueba con casos edge\n",
    "test_cases = [\n",
    "    \"Normal text\",\n",
    "    \"\",  # Texto vacío\n",
    "    None,  # No string\n",
    "    \"A\" * 15000,  # Texto muy largo\n",
    "    \"Special chars: @#$%^&*()\"\n",
    "]\n",
    "\n",
    "results = safe_sentiment_analysis(test_cases)\n",
    "for case, result in zip(test_cases, results):\n",
    "    case_preview = str(case)[:50] if case else str(case)\n",
    "    print(f\"Input: {case_preview}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicios propuestos\n",
    "\n",
    "1. **Ejercicio básico**: Implementar un clasificador de spam que procese correos electrónicos y determine si son spam o legítimos.\n",
    "\n",
    "2. **Ejercicio intermedio**: Crear un sistema que extraiga nombres de personas, organizaciones y ubicaciones de artículos de noticias y los almacene en un diccionario estructurado.\n",
    "\n",
    "3. **Ejercicio avanzado**: Desarrollar un asistente de FAQ que tome documentación técnica como contexto y responda preguntas de usuarios, con logging de confianza de las respuestas.\n",
    "\n",
    "---\n",
    "\n",
    "## Recursos adicionales\n",
    "\n",
    "- Documentación oficial: https://huggingface.co/docs/transformers\n",
    "- Model Hub: https://huggingface.co/models\n",
    "- Datasets Hub: https://huggingface.co/datasets\n",
    "- Course: https://huggingface.co/course\n",
    "- Papers with Code: https://paperswithcode.com (para entender arquitecturas base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
