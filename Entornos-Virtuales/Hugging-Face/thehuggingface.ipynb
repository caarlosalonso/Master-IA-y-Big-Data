{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masterclass: Introducción a Hugging Face para NLP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Hugging Face?\n",
    "\n",
    "### 1.1 Contextualicemos\n",
    "\n",
    "Hugging Face es una plataforma que proporciona tres componentes principales:\n",
    "\n",
    "- **Hub de modelos**\n",
    "\n",
    "- **Librería transformers**\n",
    "\n",
    "- **Ecosistema complementario**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación básica con PyTorch\n",
    "# !pip install transformers torch\n",
    "\n",
    "# Instalación con TensorFlow\n",
    "# !pip install transformers tensorflow\n",
    "\n",
    "# Instalación completa con dependencias adicionales\n",
    "# !pip install transformers[torch,sentencepiece,tokenizers]\n",
    "\n",
    "# Para trabajar con audio\n",
    "# !pip install transformers[torch,audio]\n",
    "\n",
    "# Para trabajar con visión\n",
    "# !pip install transformers[torch,vision]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.57.1\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA disponible: False\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configuración de caché\n",
    "\n",
    "Por defecto, los modelos se descargan en `~/.cache/huggingface/hub`. Para cambiar la ubicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de caché: /Documents/HuggingFace-Cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Establecer directorio de caché personalizado\n",
    "os.environ['HF_HOME'] = '/Documents/HuggingFace-Cache'\n",
    "\n",
    "# Verificar configuración\n",
    "print(f\"Directorio de caché: {os.environ.get('HF_HOME', '~/.cache/huggingface')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Búsqueda de modelos en Hugging Face\n",
    "\n",
    "### 2.1 Búsqueda manual en el Hub\n",
    "\n",
    "Navegación web: https://huggingface.co/models\n",
    "\n",
    "**Filtros disponibles:**\n",
    "\n",
    "- **Task**: text-classification, token-classification, question-answering, summarization, translation, text-generation, fill-mask, etc.\n",
    "- **Library**: transformers, sentence-transformers, spacy, flair\n",
    "- **Language**: en, es, fr, multilingual, etc.\n",
    "- **License**: apache-2.0, mit, cc-by-sa-4.0, commercial licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Búsqueda programática con Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 modelos de clasificación de texto:\n",
      "- coqui/XTTS-v2 | Descargas: 5295720\n",
      "- hexgrad/Kokoro-82M | Descargas: 4417910\n",
      "- Xenova/speecht5_tts | Descargas: 1404594\n",
      "- ResembleAI/chatterbox | Descargas: 838755\n",
      "- SWivid/F5-TTS | Descargas: 628950\n",
      "- bosonai/higgs-audio-v2-generation-3B-base | Descargas: 369579\n",
      "- myshell-ai/MeloTTS-Korean | Descargas: 284157\n",
      "- myshell-ai/MeloTTS-Japanese | Descargas: 246853\n",
      "- parler-tts/parler-tts-large-v1 | Descargas: 212065\n",
      "- nari-labs/Dia-1.6B | Descargas: 194977\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "# Crear cliente API\n",
    "api = HfApi()\n",
    "\n",
    "# Búsqueda básica: modelos de clasificación de texto\n",
    "models = list_models(\n",
    "    filter=\"text-to-speech\",\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "print(\"Top 10 modelos de clasificación de texto:\")\n",
    "for model in models:\n",
    "    print(f\"- {model.modelId} | Descargas: {model.downloads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Búsqueda avanzada con múltiples filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar modelos de NER en español\n",
    "models_ner_es = list_models(\n",
    "    filter={\"task\": \"token-classification\", \"language\": \"es\"},\n",
    "    sort=\"downloads\",\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(\"\\nModelos de NER en español:\")\n",
    "for model in models_ner_es:\n",
    "    print(f\"- {model.modelId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar modelos de traducción inglés-español\n",
    "models_translation = list_models(\n",
    "    filter=\"translation\",\n",
    "    search=\"en-es\",\n",
    "    sort=\"likes\",\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(\"\\nModelos de traducción EN-ES:\")\n",
    "for model in models_translation:\n",
    "    print(f\"- {model.modelId} | Likes: {model.likes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Categorías principales de tareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de tareas disponibles\n",
    "TASKS = {\n",
    "    # Texto\n",
    "    \"text-classification\": \"Clasificación de texto (sentimiento, spam, categorías)\",\n",
    "    \"token-classification\": \"Clasificación de tokens (NER, POS tagging)\",\n",
    "    \"question-answering\": \"Respuesta a preguntas dado un contexto\",\n",
    "    \"summarization\": \"Resumen de textos largos\",\n",
    "    \"translation\": \"Traducción entre idiomas\",\n",
    "    \"text-generation\": \"Generación de texto continuado\",\n",
    "    \"fill-mask\": \"Completar texto con máscaras\",\n",
    "    \"text2text-generation\": \"Transformación texto a texto (paráfrasis, etc.)\",\n",
    "    \n",
    "    # Audio\n",
    "    \"automatic-speech-recognition\": \"Transcripción de audio a texto\",\n",
    "    \"audio-classification\": \"Clasificación de audio (género musical, emociones)\",\n",
    "    \"text-to-speech\": \"Síntesis de voz\",\n",
    "    \n",
    "    # Visión\n",
    "    \"image-classification\": \"Clasificación de imágenes\",\n",
    "    \"object-detection\": \"Detección de objetos en imágenes\",\n",
    "    \"image-segmentation\": \"Segmentación de imágenes\",\n",
    "    \n",
    "    # Multimodal\n",
    "    \"visual-question-answering\": \"Responder preguntas sobre imágenes\",\n",
    "    \"image-to-text\": \"Generar descripciones de imágenes\"\n",
    "}\n",
    "\n",
    "for task, description in TASKS.items():\n",
    "    print(f\"{task}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Recomendaciones de modelos por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMMENDED_MODELS = {\n",
    "    # Clasificación de texto - inglés\n",
    "    \"sentiment_en\": [\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",  # Rápido, eficiente\n",
    "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\",  # Especializado en redes sociales\n",
    "    ],\n",
    "    \n",
    "    # Clasificación de texto - español\n",
    "    \"sentiment_es\": [\n",
    "        \"finiteautomata/beto-sentiment-analysis\",  # BETO adaptado\n",
    "        \"pysentimiento/robertuito-sentiment-analysis\",  # Especializado en español latinoamericano\n",
    "    ],\n",
    "    \n",
    "    # NER - inglés\n",
    "    \"ner_en\": [\n",
    "        \"dslim/bert-base-NER\",  # General purpose\n",
    "        \"dbmdz/bert-large-cased-finetuned-conll03-english\",  # Alta precisión\n",
    "    ],\n",
    "    \n",
    "    # NER - español\n",
    "    \"ner_es\": [\n",
    "        \"mrm8488/bert-spanish-cased-finetuned-ner\",\n",
    "        \"dccuchile/bert-base-spanish-wwm-cased-finetuned-conll02-spanish\",\n",
    "    ],\n",
    "    \n",
    "    # Question Answering\n",
    "    \"qa_en\": [\n",
    "        \"distilbert-base-cased-distilled-squad\",  # Eficiente\n",
    "        \"deepset/roberta-base-squad2\",  # Mayor precisión\n",
    "    ],\n",
    "    \n",
    "    # Traducción\n",
    "    \"translation\": [\n",
    "        \"Helsinki-NLP/opus-mt-en-es\",  # EN -> ES\n",
    "        \"Helsinki-NLP/opus-mt-es-en\",  # ES -> EN\n",
    "    ],\n",
    "    \n",
    "    # Summarization\n",
    "    \"summarization\": [\n",
    "        \"facebook/bart-large-cnn\",  # Noticias\n",
    "        \"t5-base\",  # Propósito general\n",
    "    ],\n",
    "    \n",
    "    # Text Generation\n",
    "    \"generation\": [\n",
    "        \"gpt2\",  # Baseline\n",
    "        \"distilgpt2\",  # Más ligero\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Función para obtener recomendaciones\n",
    "def get_recommended_model(task: str, index: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene modelo recomendado para una tarea.\n",
    "    \n",
    "    Args:\n",
    "        task: Clave de la tarea\n",
    "        index: Índice del modelo (0 = más recomendado)\n",
    "    \n",
    "    Returns:\n",
    "        Nombre del modelo\n",
    "    \"\"\"\n",
    "    if task in RECOMMENDED_MODELS:\n",
    "        return RECOMMENDED_MODELS[task][index]\n",
    "    else:\n",
    "        raise ValueError(f\"Tarea {task} no encontrada\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "model_name = get_recommended_model(\"sentiment_en\", 0)\n",
    "print(f\"Modelo recomendado para sentiment analysis: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Información detallada de un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import model_info\n",
    "\n",
    "# Obtener metadatos de un modelo\n",
    "info = model_info(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(f\"Modelo: {info.modelId}\")\n",
    "print(f\"Tarea: {info.pipeline_tag}\")\n",
    "print(f\"Descargas: {info.downloads}\")\n",
    "print(f\"Likes: {info.likes}\")\n",
    "print(f\"Licencia: {info.cardData.get('license', 'No especificada')}\")\n",
    "print(f\"Idiomas: {info.cardData.get('language', 'No especificado')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ¿Qué es un Pipeline?\n",
    "\n",
    "### 3.1 Definición técnica\n",
    "\n",
    "Un **pipeline** es una abstracción de alto nivel que encapsula tres componentes del proceso de inferencia:\n",
    "\n",
    "1. **Preprocessing (Tokenizer)**: Convierte texto crudo en tensores numéricos que el modelo puede procesar\n",
    "2. **Model**: Red neuronal que procesa los tensores y genera representaciones\n",
    "3. **Postprocessing**: Transforma las salidas del modelo en formato interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Arquitectura interna\n",
    "\n",
    "#### Proceso manual (sin pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\frasq\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha: 1\n",
      "Probabilidades: tensor([[1.3049e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. Preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "inputs = tokenizer(\"This is great!\", return_tensors=\"pt\")\n",
    "\n",
    "# 2. Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 3. Postprocessing\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "\n",
    "print(f\"Clase predicha: {predicted_class}\")\n",
    "print(f\"Probabilidades: {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mismo proceso usando pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\frasq\\Documents\\GitHub\\Master-IA-y-Big-Data\\Entornos-Virtuales\\Hugging-Face\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\frasq\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998694658279419}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"This is great!\")\n",
    "\n",
    "print(result)\n",
    "# [{'label': 'POSITIVE', 'score': 0.9998}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ventajas del pipeline\n",
    "\n",
    "- **Simplificación**: Una línea de código vs. múltiples pasos\n",
    "- **Configuración automática**: Descarga y configura tokenizer y modelo compatible\n",
    "- **Manejo de errores**: Gestión automática de casos edge (textos vacíos, muy largos, etc.)\n",
    "- **Optimizaciones**: Batch processing, device management (CPU/GPU)\n",
    "\n",
    "### 3.4 Limitaciones del pipeline\n",
    "\n",
    "- **Menos control**: No acceso a representaciones intermedias\n",
    "- **Personalización limitada**: Difícil modificar preprocessing o postprocessing\n",
    "- **Rendimiento**: Overhead en producción de alto volumen\n",
    "\n",
    "**Recomendación**: Usar pipelines para prototipado rápido y demos. Para producción, considerar implementación manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Sintaxis básica para utilizar cualquier modelo\n",
    "\n",
    "### 4.1 Sintaxis con pipeline (alto nivel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sintaxis general\n",
    "pipeline_obj = pipeline(\n",
    "    task=\"nombre-de-tarea\",              # Obligatorio\n",
    "    model=\"nombre-modelo\",                # Opcional (usa modelo default)\n",
    "    tokenizer=\"nombre-tokenizer\",         # Opcional (usa tokenizer del modelo)\n",
    "    device=0,                             # Opcional (0=GPU, -1=CPU)\n",
    "    batch_size=8,                         # Opcional (procesamiento por lotes)\n",
    "    max_length=512,                       # Opcional (longitud máxima)\n",
    "    truncation=True                       # Opcional (truncar si excede max_length)\n",
    ")\n",
    "\n",
    "# Uso\n",
    "# resultado = pipeline_obj(\"texto de entrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sintaxis manual (bajo nivel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. Cargar tokenizer y modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nombre-del-modelo\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nombre-del-modelo\")\n",
    "\n",
    "# 2. Preparar datos\n",
    "text = \"texto de entrada\"\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",      # PyTorch tensors\n",
    "    padding=True,             # Padding a longitud máxima del batch\n",
    "    truncation=True,          # Truncar si excede max_length\n",
    "    max_length=512            # Longitud máxima\n",
    ")\n",
    "\n",
    "# 3. Inferencia\n",
    "model.eval()  # Modo evaluación\n",
    "with torch.no_grad():  # Sin calcular gradientes\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 4. Procesar salida\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Clases Auto* para diferentes tareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,    # Clasificación de texto\n",
    "    AutoModelForTokenClassification,       # NER, POS tagging\n",
    "    AutoModelForQuestionAnswering,         # Question Answering\n",
    "    AutoModelForSeq2SeqLM,                 # Traducción, summarization\n",
    "    AutoModelForCausalLM,                  # Generación de texto (GPT)\n",
    "    AutoModelForMaskedLM                   # Fill-mask (BERT)\n",
    ")\n",
    "\n",
    "# Ejemplo: selección automática según arquitectura\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Las clases Auto* detectan automáticamente la arquitectura\n",
    "model_classification = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model_masked_lm = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Sintaxis para batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "results = classifier(texts)  # Procesa automáticamente en batch\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Gestión de dispositivo (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Verificar disponibilidad de CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Con pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", device=0)  # 0 = primera GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.to(device)\n",
    "\n",
    "inputs = tokenizer(\"texto\", return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}  # Mover inputs a GPU\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Ejemplos prácticos\n",
    "\n",
    "### 5.1 Ejemplo 1: Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Analizar reviews de productos\n",
    "reviews = [\n",
    "    \"This product exceeded all my expectations. Highly recommended!\",\n",
    "    \"Terrible quality. Broke after one day of use.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Best purchase I've made this year!\"\n",
    "]\n",
    "\n",
    "results = sentiment_analyzer(reviews)\n",
    "\n",
    "# Mostrar resultados\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.4f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Amazon procesa millones de reviews diariamente para calcular ratings ajustados por sentimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ejemplo 2: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline de NER\n",
    "ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    grouped_entities=True  # Agrupa tokens de la misma entidad\n",
    ")\n",
    "\n",
    "# Caso de uso: Extraer información de artículos de noticias\n",
    "text = \"\"\"\n",
    "Apple Inc. announced that CEO Tim Cook will visit the new headquarters \n",
    "in Cupertino, California next Monday. The company reported revenue of \n",
    "$394 billion in 2023.\n",
    "\"\"\"\n",
    "\n",
    "entities = ner(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "print(\"Entidades detectadas:\")\n",
    "for entity in entities:\n",
    "    print(f\"- {entity['word']}: {entity['entity_group']} (score: {entity['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Reuters utiliza NER para etiquetar automáticamente noticias y crear bases de datos de eventos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Ejemplo 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Sistema de FAQ automatizado\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications \n",
    "using machine learning. The company was founded in 2016 and is based in \n",
    "New York City. Their main product is the Transformers library, which \n",
    "provides APIs and tools to download and train state-of-the-art pretrained \n",
    "models. The library supports PyTorch, TensorFlow, and JAX.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Where is the company based?\",\n",
    "    \"What frameworks does the library support?\"\n",
    "]\n",
    "\n",
    "# Responder preguntas\n",
    "for question in questions:\n",
    "    result = qa_model(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (score: {result['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Zendesk implementa QA para responder automáticamente tickets de soporte basándose en documentación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Ejemplo 4: Traducción automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline de traducción\n",
    "translator_en_es = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-es\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Traducir documentación técnica\n",
    "english_texts = [\n",
    "    \"Machine learning models require large amounts of data for training.\",\n",
    "    \"The transformer architecture revolutionized natural language processing.\",\n",
    "    \"Python is the most popular programming language for data science.\"\n",
    "]\n",
    "\n",
    "# Traducir\n",
    "translations = translator_en_es(english_texts)\n",
    "\n",
    "# Mostrar resultados\n",
    "for original, translation in zip(english_texts, translations):\n",
    "    print(f\"EN: {original}\")\n",
    "    print(f\"ES: {translation['translation_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Microsoft Translator API utiliza arquitecturas similares para traducción en tiempo real en Teams y Skype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Ejemplo 5: Fill-mask (completar texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar pipeline\n",
    "unmasker = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "# Caso de uso: Sugerencias de autocompletado\n",
    "text_with_mask = \"The capital of France is [MASK].\"\n",
    "\n",
    "# Obtener predicciones\n",
    "predictions = unmasker(text_with_mask)\n",
    "\n",
    "# Mostrar top 5 predicciones\n",
    "print(f\"Texto: {text_with_mask}\")\n",
    "print(\"\\nPredicciones:\")\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    print(f\"{i}. {pred['token_str']}: {pred['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicación real**: Gmail utiliza modelos similares para Smart Compose (sugerencias de autocompletado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Ejemplo 6: Implementación manual con control detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentClassifier:\n",
    "    \"\"\"\n",
    "    Clasificador de sentimiento con control manual del proceso.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Inicializa el clasificador.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Nombre del modelo en Hugging Face\n",
    "            device: 'cpu' o 'cuda'\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Mapeo de etiquetas\n",
    "        self.labels = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    \n",
    "    def predict(self, text: str, return_probs: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Predice el sentimiento de un texto.\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a clasificar\n",
    "            return_probs: Si True, devuelve probabilidades de todas las clases\n",
    "            \n",
    "        Returns:\n",
    "            dict con label, score y opcionalmente probabilities\n",
    "        \"\"\"\n",
    "        # Tokenización\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Mover a dispositivo\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Inferencia\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Predicción\n",
    "        predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0][predicted_class].item()\n",
    "        \n",
    "        result = {\n",
    "            \"label\": self.labels[predicted_class],\n",
    "            \"score\": confidence\n",
    "        }\n",
    "        \n",
    "        if return_probs:\n",
    "            result[\"probabilities\"] = {\n",
    "                label: probs[0][idx].item() \n",
    "                for idx, label in self.labels.items()\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_batch(self, texts: list) -> list:\n",
    "        \"\"\"\n",
    "        Predice el sentimiento de múltiples textos.\n",
    "        \n",
    "        Args:\n",
    "            texts: Lista de textos\n",
    "            \n",
    "        Returns:\n",
    "            Lista de diccionarios con predicciones\n",
    "        \"\"\"\n",
    "        # Tokenización por lotes\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Mover a dispositivo\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Inferencia\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Procesar resultados\n",
    "        results = []\n",
    "        for i in range(len(texts)):\n",
    "            predicted_class = torch.argmax(probs[i]).item()\n",
    "            confidence = probs[i][predicted_class].item()\n",
    "            \n",
    "            results.append({\n",
    "                \"label\": self.labels[predicted_class],\n",
    "                \"score\": confidence\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso del clasificador\n",
    "\n",
    "# Inicializar\n",
    "classifier = SentimentClassifier(\n",
    "    model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Predicción individual\n",
    "result = classifier.predict(\n",
    "    \"This product is amazing!\",\n",
    "    return_probs=True\n",
    ")\n",
    "print(\"Predicción individual:\")\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "# Predicción por lotes\n",
    "texts = [\n",
    "    \"Great service and fast delivery!\",\n",
    "    \"Disappointed with the quality.\",\n",
    "    \"Average product, nothing special.\"\n",
    "]\n",
    "\n",
    "results = classifier.predict_batch(texts)\n",
    "print(\"Predicción por lotes:\")\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"{text} -> {result['label']} ({result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Ejemplo 7: Manejo de errores y casos edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def safe_sentiment_analysis(texts: list) -> list:\n",
    "    \"\"\"\n",
    "    Análisis de sentimiento con manejo de errores.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos a analizar\n",
    "        \n",
    "    Returns:\n",
    "        Lista de resultados con manejo de errores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        classifier = pipeline(\"sentiment-analysis\")\n",
    "        results = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                # Validar entrada\n",
    "                if not isinstance(text, str):\n",
    "                    logger.warning(f\"Input no es string: {type(text)}\")\n",
    "                    results.append({\"error\": \"Invalid input type\"})\n",
    "                    continue\n",
    "                \n",
    "                if len(text.strip()) == 0:\n",
    "                    logger.warning(\"Texto vacío\")\n",
    "                    results.append({\"error\": \"Empty text\"})\n",
    "                    continue\n",
    "                \n",
    "                # Truncar si es muy largo\n",
    "                if len(text) > 10000:\n",
    "                    logger.warning(f\"Texto muy largo ({len(text)} caracteres), truncando\")\n",
    "                    text = text[:10000]\n",
    "                \n",
    "                # Predecir\n",
    "                result = classifier(text)[0]\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error procesando texto: {str(e)}\")\n",
    "                results.append({\"error\": str(e)})\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inicializando pipeline: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Prueba con casos edge\n",
    "test_cases = [\n",
    "    \"Normal text\",\n",
    "    \"\",  # Texto vacío\n",
    "    None,  # No string\n",
    "    \"A\" * 15000,  # Texto muy largo\n",
    "    \"Special chars: @#$%^&*()\"\n",
    "]\n",
    "\n",
    "results = safe_sentiment_analysis(test_cases)\n",
    "for case, result in zip(test_cases, results):\n",
    "    case_preview = str(case)[:50] if case else str(case)\n",
    "    print(f\"Input: {case_preview}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicios propuestos\n",
    "\n",
    "1. **Ejercicio básico**: Implementar un clasificador de spam que procese correos electrónicos y determine si son spam o legítimos.\n",
    "\n",
    "2. **Ejercicio intermedio**: Crear un sistema que extraiga nombres de personas, organizaciones y ubicaciones de artículos de noticias y los almacene en un diccionario estructurado.\n",
    "\n",
    "3. **Ejercicio avanzado**: Desarrollar un asistente de FAQ que tome documentación técnica como contexto y responda preguntas de usuarios, con logging de confianza de las respuestas.\n",
    "\n",
    "---\n",
    "\n",
    "## Recursos adicionales\n",
    "\n",
    "- Documentación oficial: https://huggingface.co/docs/transformers\n",
    "- Model Hub: https://huggingface.co/models\n",
    "- Datasets Hub: https://huggingface.co/datasets\n",
    "- Course: https://huggingface.co/course\n",
    "- Papers with Code: https://paperswithcode.com (para entender arquitecturas base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
